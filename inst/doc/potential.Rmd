---
title: "Unleashing the power of R for event data"
author: "Gert Janssenswillen"
date: 2/12/2015
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Unleashing the power of R for event data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Not only does edeaR allow one to hande, describe and filter event data in a convenient way, it provides the analyst with a broad spectrum of analysis techniques which are already available within R. This vignette will highlight this by means of some simple though illustrative examples. The examples in this vignette will used the event log of the BPI Challenge 2014. More specifically, a sample of incident activities with their corresponding case attributes will be used. Both data sets are available within the packages.

```{r message = F, warning = F}
library(edeaR)
library(dplyr)
library(ggplot2)
data("BPIC14_incident_log")
data("BPIC14_incident_case_attributes")
BPIC14_incident_log %>% print
BPIC14_incident_case_attributes %>% print
```

Suppose we are interested in inspecting the structuredness of this event log, and any possible relationship with performance. A first inspection is just to look at the number of different traces, i.e. variants in terms of activity sequences, which are present in the event log. 

```{r}
trace_coverage(BPIC14_incident_log, level_of_analysis = "trace") %>% print(width = Inf)
```

The output shows that the 10 most common activity sequences are able to cover about 20\% of the cases in the log. However, there are a total of 2430 different traces in the event log. Thus, it is clear that there is a fair amount of unstructuredness. Moreover, it seems that less frequent traces are long, in terms of the number of activity instance. The graph below shows that the activity sequences which occur less that occur more than once, remain quite short. However, there exists a lot of exceptional traces which get very long, i.e. up to 170 activity instances.

```{r fig.width = 7}
trace_length(BPIC14_incident_log, "trace") %>% ggplot(aes(relative_trace_frequency, absolute)) + geom_jitter() + scale_x_continuous(limits = c(0,0.01)) + ylab("Trace length") + xlab("Relative trace frequency")
```

An interesting analysis would therefore be to define a performance _vector_ for each case in order to identify bad performance cases and examine them more closely, e.g., by looking at their case attributes. For the sake of simplicity, predefined measures will be used to quantify performance. However, any self-defined property of cases can be used. The case properties used in this example are trace length, throughput time, number of self-loops and the number of repetitions. The following piece of code will compute these measures and combine them in one table.

```{r}
case_performance <- BPIC14_incident_log %>% throughput_time("case") %>% 
	left_join(BPIC14_incident_log %>% trace_length("case")) %>% 
	left_join(BPIC14_incident_log %>% repetitions("case") %>% select(incident_id, absolute) %>% rename(repetitions = absolute)) %>% 
	left_join(BPIC14_incident_log %>% number_of_selfloops("case") %>% select(incident_id, absolute) %>% rename(number_of_selfloops = absolute)) 

case_performance %>% summary
```

### Cluster analysis

The overal summary shows that there is a wide diversity on each defined aspect of performance. Moreover, it is clear that all variables are right skewed, due to the existence of a limited number of bad performing cases. A cluster analysis might be able to distinguish bad performance cases based on these values. However, since the data is highly skewed, the variables were first normalized. To decide upon the number of clusters, we performed a various number of clusterings, each with a different number of clusters, and compared the SSE of each clustering. To control for the randomness in the selection of centres, 100 different iterations were done at each moment. The graph shows the minimum SSE that was seen for each number of clusters.

```{r fig.width = 7, fig.align='center'}
input <- scale(case_performance[,2:5]) %>% as.data.frame()

clusters <- data.frame(i = 1:15)
for(i in 1:nrow(clusters)) {
	for(j in 1:100) {
		cl <- kmeans(input,i, iter.max = 20)
		min_sse <- min(Inf, cl$totss - cl$betweenss)
	}
	clusters$sse[i] <- min_sse
}
clusters %>% ggplot(aes(i, sse)) + geom_line() +
	xlab("Number of clusters") + ylab("SSE") + 
	scale_x_continuous(breaks = 1:15) + scale_y_continuous(breaks = seq(0,16000,2000))
```

It can be observed that the in SSE is negligible when the number of clusters is higher than 5. Therefore, 5 seems to be a reasonable number of clusters. The output of the final clustering belows show that the 5 resulting clusters differ reasonably in size: there is one major cluster, containg about 77\% of the cases, and another smaller clusters containing another 17\%. The remaining 6\% of cases are divided into three tiny clusters. However, keeping in mind the skewedness of the data, this result is not that surprising.

```{r}
set.seed(4)
cl <- kmeans(input,5, iter.max = 20)
case_performance <- case_performance %>% bind_cols(data.frame(cluster = factor(cl$cluster)))
cl %>% str
```

The table and figures below show how the different clusters are characterised by the different variables. The main cluster, i.e. cluster 4, contains cases which score low on all 4 measures. On the other side of the spectrum, cluster 5 contains cases which on average have a very high value for all metrics. Note however that this is the smallest cluster, thereby really covering exceptional behaviour. Cluster 2 is more or less similar to cluster 5 concerningthe number of repetitions and the trace length. However, the cases in this cluster have less self-loops and remarkably lower throughput times. The two remaining clusters, 1 and 3, contain cases that score reasonably good on all aspects, though inferior to cluster 4. Among these two, cluster 1 seems to be superior.

### Explaining the clusters

We have now divided all cases in groups with similar performance characteristics. Subsequently, it would be interesting to see whether these performance characteristics are connected with other attributes, related to the incidents itself. Therefore, we connect the clustering output with the case attributes. 

```{r fig.align="center", fig.width= 7, echo = F}
case_performance %>%
	group_by(cluster) %>% 
	summarize(freq = n(),
			  mean_nr_of_repetitions = mean(repetitions),
			  mean_nr_of_selfloops = mean(number_of_selfloops),
			  mean_trace_length = mean(trace_length),
			  mean_throughput_time = mean(throughput_time)) %>%
	print(width = Inf)

case_performance %>% ggplot(aes(cluster, repetitions)) + geom_boxplot(aes(fill = cluster))
case_performance %>% ggplot(aes(cluster, number_of_selfloops)) + geom_boxplot(aes(fill = cluster))
case_performance %>% ggplot(aes(cluster, trace_length)) + geom_boxplot(aes(fill = cluster))
case_performance %>% ggplot(aes(cluster, throughput_time)) + geom_boxplot(aes(fill = cluster))
```


```{r}
BPIC14_incident_case_attributes <- BPIC14_incident_case_attributes %>%
	merge(case_performance) 
```

The graph below that for some configuration item types, the number of cases which performe good at the selected measures is relatively lower that for others. For instance, for incidents related to subapplications, about 80\% belongs to the _high-performing_ cluster (4), while for network components this is only about 50%
```{r fig.width = 7}
BPIC14_incident_case_attributes %>% ggplot(aes(reorder(ci_type_aff, as.numeric(cluster) == 4, FUN = "mean"), fill = cluster)) + geom_bar(position = "fill")  +
	scale_fill_brewer() + coord_flip() + xlab("ci_type_aff") + scale_y_continuous(breaks = seq(0,1,0.1))
```
The following graph shows that cases with bad performance scores (2 and 5) typically are reassigned a lot of times, while cases with good performance levels (cluster 4) have zero are only a few reassignments.
```{r fig.width = 7}
BPIC14_incident_case_attributes %>% ggplot(aes(cluster, x_reassignments)) + geom_boxplot()  +
	scale_fill_brewer() + coord_flip() 
```
The closure code, stating in which way the incident was _solved_ also seem to be related in a certain sense to performance. Incidents were the users didn't read the user manual were often scored good on performance levels, while cases which had to be referred had typically a bad performance.
```{r fig.width = 7}
BPIC14_incident_case_attributes %>% ggplot(aes(reorder(closure_code, as.numeric(cluster) == 4, FUN = "mean"), fill = cluster)) + geom_bar(position = "fill")  +
	scale_fill_brewer() + coord_flip() + xlab("closure_code") + scale_y_continuous(breaks = seq(0,1,0.1))
```

## Subsetting the eventlog

Instead of looking at case attributes, it is also possible to filter out the bad performaning cases, for instance cluster 5.

```{r}
cluster_5 <- BPIC14_incident_log %>% 
	merge(case_performance) %>%
	filter(cluster == 5) %>%
	eventlog(case_id(BPIC14_incident_log), 
			 activity_id(BPIC14_incident_log),
			 activity_instance_id(BPIC14_incident_log),
			 lifecycle_id(BPIC14_incident_log),
			 timestamp(BPIC14_incident_log))
```

When printing the filtered event log, it can be seen that all 22 cases have a different trace, i.e. activity sequence, which makes them unique in that sense. A visualization of the behaviour in a directed graph might be helpful to understand it better. The reader is refered to packages like `igraph` of `markovchain` for this visualization.
```{r}
cluster_5 
cluster_5 %>% traces
```














